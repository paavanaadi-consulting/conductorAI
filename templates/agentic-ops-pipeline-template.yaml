# =============================================================================
# Agentic Operations Pipeline Requirements Template
# =============================================================================
# Specialized template for autonomous agent-based operations including
# multi-agent orchestration, agent collaboration, and self-healing systems.
# =============================================================================

# -----------------------------------------------------------------------------
# 1. PROJECT METADATA
# -----------------------------------------------------------------------------
project:
  name: "autonomous-devops-agents"
  version: "1.0.0"
  description: |
    Multi-agent system for autonomous DevOps operations including
    incident detection, root cause analysis, automated remediation,
    and continuous optimization. Agents collaborate to maintain
    system health with minimal human intervention.
  
  agent_architecture: "multi_agent_collaborative"  # single_agent | multi_agent_collaborative |
                                                   # hierarchical | swarm | hybrid
  
  orchestration_pattern: "conductor"  # conductor | choreography | event_driven | hybrid
  
  domain: "devops_automation"
  priority: "high"
  deadline: "2026-08-31"
  
  stakeholders:
    product_owner: "platform-eng@company.com"
    agent_architect: "ai-eng@company.com"
    devops_lead: "devops@company.com"
    team: "platform-engineering"

# -----------------------------------------------------------------------------
# 2. AGENT SYSTEM OBJECTIVES
# -----------------------------------------------------------------------------
agent_objectives:
  mission: |
    Build autonomous agent system that monitors infrastructure,
    detects anomalies, diagnoses issues, executes remediations,
    and learns from outcomes to continuously improve reliability.
  
  success_metrics:
    - metric: "mttr_reduction"
      target: "Reduce Mean Time To Recovery by 70%"
      baseline: "45 minutes (manual)"
      target_value: "< 15 minutes (automated)"
    
    - metric: "incident_auto_resolution_rate"
      target: ">= 60% of incidents resolved without human intervention"
      measurement: "auto_resolved / total_incidents"
    
    - metric: "false_positive_rate"
      target: "< 5% false alerts requiring rollback"
      measurement: "rollbacks / total_actions"
    
    - metric: "system_uptime"
      target: "Improve from 99.5% to 99.95%"
      measurement: "uptime_percentage"
  
  autonomous_capabilities:
    - capability: "Anomaly Detection"
      description: "Detect infrastructure and application anomalies"
      autonomy_level: "fully_autonomous"
      human_approval: false
    
    - capability: "Root Cause Analysis"
      description: "Analyze logs, metrics, traces to identify root cause"
      autonomy_level: "autonomous_with_confidence_threshold"
      human_approval: "required_if_confidence < 0.8"
    
    - capability: "Automated Remediation"
      description: "Execute fixes (restart services, scale resources, etc.)"
      autonomy_level: "semi_autonomous"
      human_approval: "required_for_production"
      approval_timeout: "5 minutes"
    
    - capability: "Continuous Learning"
      description: "Learn from past incidents to improve future responses"
      autonomy_level: "fully_autonomous"
      human_approval: false

# -----------------------------------------------------------------------------
# 3. MULTI-AGENT COMMUNICATION PROTOCOLS
# -----------------------------------------------------------------------------
# Define how agents communicate, coordinate, and collaborate in the system.
# Supports: Agent-to-Agent (A2A), Message Bus, Hybrid architectures
# -----------------------------------------------------------------------------
communication_architecture:
  # Overall architecture pattern
  architecture_pattern: "hybrid"  # Options: "a2a" | "message_bus" | "hybrid" | "hierarchical"
  
  description: |
    Hybrid architecture combining:
    - A2A (gRPC) for latency-sensitive operations (monitoring → diagnostics)
    - Message Bus (Kafka) for event distribution and async workflows
    - Service mesh for traffic management and observability
  
  # ---------------------------------------------------------------------------
  # Agent-to-Agent (A2A) Communication
  # ---------------------------------------------------------------------------
  a2a_protocol:
    enabled: true
    primary_transport: "grpc"  # Options: "grpc" | "http" | "websocket" | "rest"
    
    # gRPC Configuration
    grpc_config:
      host: "0.0.0.0"
      port_range: "50051-50060"
      max_message_size_mb: 100
      keepalive_time_sec: 30
      keepalive_timeout_sec: 10
      keepalive_permit_without_calls: true
      
      compression: "gzip"  # Options: "gzip" | "deflate" | "snappy" | "none"
      
      retry_policy:
        enabled: true
        max_attempts: 3
        initial_backoff_ms: 100
        max_backoff_ms: 5000
        backoff_multiplier: 2.0
        retryable_status_codes: ["UNAVAILABLE", "DEADLINE_EXCEEDED"]
      
      timeout_policy:
        default_timeout_ms: 5000
        idle_timeout_ms: 300000  # 5 minutes
        max_connection_age_ms: 1800000  # 30 minutes
    
    # HTTP/REST Configuration (fallback)
    http_config:
      enabled: false
      port: 8080
      protocol: "http2"  # Options: "http1.1" | "http2" | "http3"
      tls_enabled: true
      timeout_ms: 10000
    
    # Service Mesh Integration
    service_mesh:
      enabled: true
      implementation: "istio"  # Options: "istio" | "linkerd" | "consul" | "none"
      
      features:
        mutual_tls: true
        automatic_retries: true
        circuit_breaking: true
        fault_injection: false  # Only for testing
        
      circuit_breaker:
        max_connections: 1000
        max_pending_requests: 1000
        max_requests: 1000
        max_retries: 3
        consecutive_errors: 5
        interval_sec: 30
        base_ejection_time_sec: 30
    
    # Communication Patterns
    patterns:
      - pattern: "request_response"
        description: "Synchronous request-reply"
        use_cases:
          - "MonitoringAgent → DiagnosticAgent (anomaly analysis)"
          - "DiagnosticAgent → RemediationAgent (remediation request)"
        timeout_ms: 5000
        retry_enabled: true
        
      - pattern: "streaming"
        description: "Unidirectional stream"
        use_cases:
          - "MonitoringAgent → DiagnosticAgent (real-time metrics stream)"
        timeout_ms: 300000  # 5 minutes
        buffer_size: 1000
        
      - pattern: "bidirectional_streaming"
        description: "Two-way streaming communication"
        use_cases:
          - "DiagnosticAgent ↔ LearningAgent (interactive troubleshooting)"
        timeout_ms: 600000  # 10 minutes
        buffer_size: 500
    
    # Service Discovery
    service_discovery:
      mechanism: "kubernetes_dns"  # Options: "kubernetes_dns" | "consul" | "etcd" | "eureka"
      namespace: "agentic-ops"
      service_name_pattern: "{agent_id}-service.{namespace}.svc.cluster.local"
      
      health_checks:
        enabled: true
        protocol: "grpc_health_v1"  # gRPC health checking protocol
        interval_sec: 30
        timeout_sec: 5
        unhealthy_threshold: 3
        healthy_threshold: 2
      
      dns_config:
        ndots: 5
        timeout_sec: 2
        attempts: 3

  # ---------------------------------------------------------------------------
  # Message Bus Communication
  # ---------------------------------------------------------------------------
  message_bus:
    enabled: true
    broker_type: "kafka"  # Options: "kafka" | "rabbitmq" | "redis_streams" | "pulsar" | "nats"
    
    # Kafka Configuration
    kafka_config:
      bootstrap_servers:
        - "kafka-1.agentic-ops.svc.cluster.local:9092"
        - "kafka-2.agentic-ops.svc.cluster.local:9092"
        - "kafka-3.agentic-ops.svc.cluster.local:9092"
      
      cluster_settings:
        replication_factor: 3
        min_in_sync_replicas: 2
        default_partitions: 16
        retention_hours: 168  # 7 days
        retention_bytes: 1073741824  # 1GB per partition
        compression_type: "snappy"  # Options: "snappy" | "gzip" | "lz4" | "zstd"
        
      producer_config:
        acks: "all"  # Wait for all replicas
        max_in_flight_requests: 5
        batch_size_bytes: 16384
        linger_ms: 10
        compression_type: "snappy"
        enable_idempotence: true
        max_request_size_bytes: 1048576  # 1MB
        
      consumer_config:
        group_id_prefix: "agentic_ops"
        auto_offset_reset: "latest"  # Options: "earliest" | "latest"
        enable_auto_commit: false  # Manual commit for reliability
        max_poll_records: 500
        max_poll_interval_ms: 300000
        session_timeout_ms: 30000
        heartbeat_interval_ms: 3000
        fetch_min_bytes: 1024
        fetch_max_wait_ms: 500
    
    # RabbitMQ Configuration (alternative)
    rabbitmq_config:
      enabled: false
      host: "rabbitmq.agentic-ops.svc.cluster.local"
      port: 5672
      vhost: "/"
      username: "${RABBITMQ_USER}"
      password: "${RABBITMQ_PASSWORD}"
      
      exchange_config:
        type: "topic"  # Options: "direct" | "topic" | "fanout" | "headers"
        durable: true
        auto_delete: false
      
      queue_config:
        durable: true
        exclusive: false
        auto_delete: false
        max_length: 100000
        message_ttl_ms: 604800000  # 7 days
    
    # Topic Taxonomy
    topics:
      # Event Topics (Pub/Sub)
      events:
        - name: "incidents.detected"
          description: "Monitoring agent publishes detected incidents"
          partitions: 16
          replication_factor: 3
          publishers: ["monitoring_agent"]
          subscribers: ["diagnostic_agent", "notification_agent", "learning_agent"]
          message_format: "avro"
          schema_file: "schemas/incident_detected.avsc"
          retention_hours: 168
          
        - name: "diagnostics.completed"
          description: "Diagnostic agent publishes RCA results"
          partitions: 8
          publishers: ["diagnostic_agent"]
          subscribers: ["remediation_agent", "learning_agent", "notification_agent"]
          message_format: "avro"
          schema_file: "schemas/diagnostic_completed.avsc"
          
        - name: "remediation.executed"
          description: "Remediation actions taken"
          partitions: 8
          publishers: ["remediation_agent"]
          subscribers: ["monitoring_agent", "learning_agent", "audit_agent"]
          message_format: "protobuf"
          schema_file: "schemas/remediation_executed.proto"
          
        - name: "remediation.failed"
          description: "Remediation failures requiring escalation"
          partitions: 4
          publishers: ["remediation_agent"]
          subscribers: ["notification_agent", "escalation_agent"]
          message_format: "json"
          
        - name: "learning.insights"
          description: "Learning agent publishes improved patterns"
          partitions: 4
          publishers: ["learning_agent"]
          subscribers: ["diagnostic_agent", "remediation_agent"]
          message_format: "avro"
      
      # Command Topics (Point-to-Point)
      commands:
        - name: "remediation.requests"
          description: "Commands for remediation agent"
          partitions: 8
          consumer_group: "remediation_agent_group"
          message_format: "avro"
          schema_file: "schemas/remediation_request.avsc"
          
        - name: "escalation.requests"
          description: "Human escalation requests"
          partitions: 2
          consumer_group: "escalation_agent_group"
          message_format: "json"
      
      # Dead Letter Queues
      dead_letter:
        - name: "incidents.dlq"
          description: "Failed incident processing"
          partitions: 1
          retention_hours: 720  # 30 days
    
    # Messaging Patterns
    patterns:
      - pattern: "publish_subscribe"
        description: "Event broadcasting to multiple subscribers"
        use_cases:
          - "incidents.detected → all interested agents subscribe"
          - "diagnostics.completed → remediation + learning agents"
        delivery_guarantee: "at_least_once"
        ordering_guarantee: "partition_level"
        
      - pattern: "request_reply_async"
        description: "Async request-reply with correlation IDs"
        use_cases:
          - "Diagnostic agent requests historical data from learning agent"
        delivery_guarantee: "exactly_once"
        correlation_id_header: "correlation_id"
        reply_to_header: "reply_topic"
        timeout_ms: 60000
        
      - pattern: "event_sourcing"
        description: "All state changes as immutable events"
        use_cases:
          - "Complete incident lifecycle tracking"
          - "Audit trail for compliance"
        delivery_guarantee: "exactly_once"
        compaction_enabled: false
        retention_policy: "infinite"
        
      - pattern: "saga_orchestration"
        description: "Multi-step workflows with compensation"
        use_cases:
          - "Complex remediation requiring multiple agents"
        coordinator: "orchestrator_agent"
        compensation_enabled: true
        
  # ---------------------------------------------------------------------------
  # Protocol Selection Rules
  # ---------------------------------------------------------------------------
  protocol_selection:
    # When to use A2A (gRPC)
    use_a2a_when:
      conditions:
        - "Latency requirement < 100ms"
        - "Synchronous response required immediately"
        - "Streaming data (metrics, logs) in real-time"
        - "Bidirectional communication needed"
        - "Message size < 1MB"
      
      examples:
        - scenario: "Monitoring agent detects anomaly spike"
          communication: "A2A gRPC streaming"
          rationale: "Real-time metrics feed to diagnostic agent"
          latency_target: "< 50ms"
          
        - scenario: "Diagnostic agent requests immediate remediation approval"
          communication: "A2A gRPC request-response"
          rationale: "Human-in-loop requires immediate callback"
          latency_target: "< 100ms"
    
    # When to use Message Bus (Kafka)
    use_message_bus_when:
      conditions:
        - "Fan-out to multiple subscribers required"
        - "Asynchronous processing acceptable"
        - "Ordered delivery within partition needed"
        - "Guaranteed delivery (at-least-once or exactly-once)"
        - "Event persistence for replay/audit"
        - "Decoupling agents temporally"
        - "Message size can be large (>1MB)"
      
      examples:
        - scenario: "Monitoring agent detects incident"
          communication: "Kafka pub/sub"
          rationale: "Multiple agents need to know (diagnostic, notification, learning)"
          delivery: "at-least-once"
          
        - scenario: "Diagnostic agent completes RCA"
          communication: "Kafka event"
          rationale: "Async workflows, audit trail, learning dataset"
          delivery: "exactly-once"
          
        - scenario: "Learning agent updates incident patterns"
          communication: "Kafka event"
          rationale: "Other agents consume when ready, no blocking"
          delivery: "at-least-once"
  
  # ---------------------------------------------------------------------------
  # Agent Collaboration Workflows
  # ---------------------------------------------------------------------------
  workflows:
    - workflow_id: "incident_detection_to_resolution"
      description: "End-to-end incident handling workflow"
      orchestration_type: "choreography"  # Decentralized, event-driven
      
      steps:
        - step: 1
          agent: "monitoring_agent"
          action: "detect_anomaly"
          trigger: "metrics_threshold_exceeded"
          communication: "message_bus"
          output_topic: "incidents.detected"
          
        - step: 2
          agent: "diagnostic_agent"
          action: "analyze_root_cause"
          trigger: "incidents.detected"
          communication: "a2a"  # Fast analysis with streaming metrics
          input_protocol: "grpc_streaming"
          output_topic: "diagnostics.completed"
          sla_ms: 30000  # 30 seconds
          
        - step: 3
          agents: ["remediation_agent", "learning_agent", "notification_agent"]
          action: "parallel_processing"
          trigger: "diagnostics.completed"
          communication: "message_bus"
          pattern: "fan_out"
          
        - step: 4
          agent: "remediation_agent"
          action: "execute_remediation"
          trigger: "diagnostics.completed"
          communication: "message_bus"
          approval_required: true
          approval_protocol: "a2a"  # Fast human approval via gRPC
          output_topic: "remediation.executed"
          
        - step: 5
          agent: "learning_agent"
          action: "update_knowledge_base"
          trigger: "remediation.executed"
          communication: "message_bus"
          async: true
          output_topic: "learning.insights"
    
    - workflow_id: "proactive_optimization"
      description: "Continuous learning and optimization workflow"
      orchestration_type: "orchestration"  # Centralized coordinator
      coordinator: "orchestrator_agent"
      
      steps:
        - step: 1
          agent: "learning_agent"
          action: "analyze_historical_incidents"
          communication: "message_bus"
          schedule: "0 2 * * *"  # Daily at 2 AM
          
        - step: 2
          agent: "learning_agent"
          action: "generate_optimization_recommendations"
          communication: "a2a"  # Sync request to orchestrator
          output: "optimization_proposals"
          
        - step: 3
          agent: "orchestrator_agent"
          action: "validate_and_apply_optimizations"
          communication: "message_bus"
          approval_required: true

  # ---------------------------------------------------------------------------
  # Message Serialization & Schema Management
  # ---------------------------------------------------------------------------
  serialization:
    default_format: "avro"  # Options: "avro" | "protobuf" | "json" | "msgpack"
    
    # Schema Registry (for Avro/Protobuf)
    schema_registry:
      enabled: true
      implementation: "confluent_schema_registry"
      url: "http://schema-registry.agentic-ops.svc.cluster.local:8081"
      
      compatibility_mode: "backward"  # Options: "backward" | "forward" | "full" | "none"
      
      auto_register_schemas: false  # Require explicit schema registration
      
      schema_validation:
        enabled: true
        fail_on_invalid_schema: true
    
    # Format-specific configuration
    formats:
      avro:
        enabled: true
        codec: "null"  # Options: "null" | "deflate" | "snappy"
        schema_location: "schemas/avro/"
        
      protobuf:
        enabled: true
        schema_location: "schemas/proto/"
        
      json:
        enabled: true
        pretty_print: false
        schema_validation: true  # JSON Schema validation
        schema_location: "schemas/json/"
    
    # Schema Evolution Strategy
    evolution:
      strategy: "backward_compatible"
      versioning: "semantic"
      deprecation_policy: "6_months"
      breaking_change_approval: "required"

  # ---------------------------------------------------------------------------
  # Coordination & Consensus
  # ---------------------------------------------------------------------------
  coordination:
    # Distributed Consensus
    consensus:
      enabled: true
      protocol: "raft"  # Options: "raft" | "paxos" | "viewstamped_replication"
      backend: "etcd"
      
      etcd_config:
        endpoints:
          - "etcd-1.agentic-ops.svc.cluster.local:2379"
          - "etcd-2.agentic-ops.svc.cluster.local:2379"
          - "etcd-3.agentic-ops.svc.cluster.local:2379"
        dial_timeout_sec: 5
        request_timeout_sec: 10
    
    # Leader Election
    leader_election:
      enabled: true
      mechanism: "kubernetes_lease"  # Options: "kubernetes_lease" | "etcd" | "redis"
      
      lease_config:
        namespace: "agentic-ops"
        lease_duration_sec: 15
        renew_deadline_sec: 10
        retry_period_sec: 2
      
      leader_responsibilities:
        - "Workflow orchestration"
        - "Global state management"
        - "Conflict resolution"
    
    # Distributed Locking
    distributed_locks:
      enabled: true
      backend: "redis"  # Options: "redis" | "etcd" | "zookeeper"
      
      redis_config:
        host: "redis.agentic-ops.svc.cluster.local"
        port: 6379
        db: 0
        
      lock_config:
        default_ttl_sec: 30
        retry_delay_ms: 100
        retry_count: 3
      
      use_cases:
        - "Remediation action coordination (prevent duplicate fixes)"
        - "Knowledge base updates (prevent conflicts)"
        - "Resource allocation (prevent over-provisioning)"

  # ---------------------------------------------------------------------------
  # Security & Authentication
  # ---------------------------------------------------------------------------
  security:
    # Encryption
    encryption:
      in_transit:
        protocol: "tls_1_3"
        cipher_suites:
          - "TLS_AES_256_GCM_SHA384"
          - "TLS_CHACHA20_POLY1305_SHA256"
        
      at_rest:
        enabled: true
        algorithm: "aes_256_gcm"
        key_rotation_days: 90
    
    # Authentication
    authentication:
      mechanism: "mutual_tls"  # Options: "mutual_tls" | "jwt" | "oauth2" | "api_key"
      
      mtls_config:
        ca_cert_path: "/etc/certs/ca.crt"
        cert_path: "/etc/certs/agent.crt"
        key_path: "/etc/certs/agent.key"
        verify_client_cert: true
        certificate_rotation_days: 90
      
      jwt_config:
        enabled: false
        issuer: "agentic-ops-auth"
        algorithm: "RS256"
        expiration_min: 60
    
    # Authorization
    authorization:
      mechanism: "rbac"  # Options: "rbac" | "abac" | "opa"
      
      rbac_config:
        roles:
          - role: "monitoring_agent"
            permissions:
              - "publish:incidents.detected"
              - "subscribe:learning.insights"
          
          - role: "remediation_agent"
            permissions:
              - "publish:remediation.executed"
              - "subscribe:diagnostics.completed"
              - "execute:remediation_actions"
      
      opa_config:
        enabled: false
        policy_url: "http://opa.agentic-ops.svc.cluster.local:8181"
    
    # Message Signing
    message_signing:
      enabled: true
      algorithm: "ed25519"
      
      signing_config:
        sign_all_messages: false  # Only sign critical messages
        sign_patterns:
          - "remediation.*"
          - "escalation.*"
        
        verification:
          verify_signatures: true
          reject_unsigned: false  # Warn but don't reject

  # ---------------------------------------------------------------------------
  # Observability & Monitoring
  # ---------------------------------------------------------------------------
  observability:
    # Distributed Tracing
    tracing:
      enabled: true
      implementation: "jaeger"  # Options: "jaeger" | "zipkin" | "tempo"
      
      jaeger_config:
        agent_host: "jaeger-agent.observability.svc.cluster.local"
        agent_port: 6831
        sampler_type: "probabilistic"
        sampler_param: 0.1  # 10% sampling
        
      trace_context:
        propagation_format: "w3c_trace_context"  # W3C Trace Context standard
        baggage_enabled: true
      
      instrumentation:
        auto_instrument: true
        custom_spans:
          - "agent_decision_making"
          - "llm_inference"
          - "remediation_execution"
    
    # Metrics
    metrics:
      enabled: true
      backend: "prometheus"
      
      protocol_metrics:
        a2a:
          - "grpc_request_duration_seconds"
          - "grpc_request_total"
          - "grpc_request_errors_total"
          - "grpc_stream_messages_sent"
          - "grpc_stream_messages_received"
          
        message_bus:
          - "kafka_producer_record_send_total"
          - "kafka_producer_record_error_total"
          - "kafka_consumer_records_consumed_total"
          - "kafka_consumer_lag"
          - "kafka_consumer_commit_latency_avg"
      
      custom_metrics:
        - "agent_decision_latency_seconds"
        - "incident_detection_to_resolution_seconds"
        - "remediation_success_rate"
        - "agent_collaboration_count"
      
      export:
        prometheus_port: 9090
        scrape_interval_sec: 15
    
    # Logging
    logging:
      enabled: true
      backend: "elasticsearch"
      
      log_config:
        level: "info"  # Options: "debug" | "info" | "warn" | "error"
        format: "json"
        
        include_fields:
          - "timestamp"
          - "agent_id"
          - "workflow_id"
          - "trace_id"
          - "span_id"
          - "communication_protocol"
          - "message_id"
          - "correlation_id"
      
      message_logging:
        log_messages: true
        log_payload: false  # Don't log full payload (security/performance)
        sample_rate: 0.01  # Log 1% of messages
      
      retention:
        days: 30
        archive_after_days: 90

  # ---------------------------------------------------------------------------
  # Performance & Optimization
  # ---------------------------------------------------------------------------
  performance:
    # Connection Pooling
    connection_pooling:
      a2a:
        pool_size: 100
        max_idle_connections: 50
        connection_timeout_ms: 5000
        
      message_bus:
        producer_pool_size: 10
        consumer_pool_size: 5
    
    # Batching
    batching:
      enabled: true
      
      message_bus:
        batch_size: 100
        batch_timeout_ms: 100
        compression_enabled: true
    
    # Caching
    caching:
      enabled: true
      backend: "redis"
      
      cache_config:
        ttl_sec: 300  # 5 minutes
        max_size_mb: 1024  # 1GB
        
      cache_patterns:
        - "Agent service discovery results"
        - "Schema registry lookups"
        - "Historical incident patterns"
    
    # Rate Limiting
    rate_limiting:
      enabled: true
      
      limits:
        - agent: "monitoring_agent"
          protocol: "message_bus"
          limit: "1000 messages/second"
          
        - agent: "diagnostic_agent"
          protocol: "a2a"
          limit: "500 requests/second"
      
      strategy: "token_bucket"  # Options: "token_bucket" | "leaky_bucket" | "fixed_window"

# -----------------------------------------------------------------------------
# 4. AGENT SPECIFICATIONS
# -----------------------------------------------------------------------------
agents:
  # Monitoring Agent
  - agent_id: "monitoring_agent"
    name: "Vigilance"
    role: "System Monitoring & Anomaly Detection"
    
    responsibilities:
      - "Monitor infrastructure metrics (CPU, memory, disk, network)"
      - "Monitor application metrics (latency, error rate, throughput)"
      - "Detect anomalies using ML models"
      - "Trigger alerts when anomalies exceed thresholds"
    
    inputs:
      - source: "prometheus"
        type: "metrics"
        metrics: ["cpu_usage", "memory_usage", "error_rate", "latency"]
        scrape_interval: "15s"
      
      - source: "cloudwatch"
        type: "metrics"
        metrics: ["ec2_metrics", "rds_metrics", "alb_metrics"]
        poll_interval: "1m"
    
    outputs:
      - type: "incident_alert"
        destination: "message_bus"
        topic: "incidents.detected"
        schema:
          incident_id: "string"
          severity: "critical|high|medium|low"
          affected_services: "array"
          metrics: "object"
          timestamp: "datetime"
    
    llm_integration:
      provider: "openai"
      model: "gpt-4"
      use_cases:
        - "Analyze metric patterns to determine severity"
        - "Generate human-readable incident descriptions"
        - "Suggest potential related services"
      
    decision_logic:
      anomaly_detection:
        algorithm: "isolation_forest"
        confidence_threshold: 0.85
        window_size: "15 minutes"
      
      severity_classification:
        critical: "error_rate > 5% OR latency_p99 > 5s OR service_down"
        high: "error_rate > 2% OR latency_p99 > 2s"
        medium: "error_rate > 1% OR latency_p95 > 1s"
        low: "minor_degradation"
    
    autonomy:
      level: "fully_autonomous"
      human_in_loop: false
      escalation: "Only for critical incidents"
  
  # Diagnostic Agent
  - agent_id: "diagnostic_agent"
    name: "Sherlock"
    role: "Root Cause Analysis & Diagnostics"
    
    responsibilities:
      - "Receive incident alerts from monitoring agent"
      - "Collect relevant logs, traces, and metrics"
      - "Perform root cause analysis using correlation and LLM"
      - "Generate diagnostic report with confidence score"
    
    inputs:
      - source: "message_bus"
        topic: "incidents.detected"
        type: "incident_alert"
      
      - source: "elasticsearch"
        type: "logs"
        query_window: "last_30_minutes"
      
      - source: "jaeger"
        type: "distributed_traces"
        query_window: "last_30_minutes"
    
    outputs:
      - type: "diagnostic_report"
        destination: "message_bus"
        topic: "diagnostics.completed"
        schema:
          incident_id: "string"
          root_cause: "string"
          confidence: "float (0-1)"
          evidence: "array of objects"
          suggested_remediation: "array"
    
    llm_integration:
      provider: "openai"
      model: "gpt-4"
      use_cases:
        - "Analyze log patterns to identify errors"
        - "Correlate multiple data sources (logs, metrics, traces)"
        - "Generate root cause hypothesis"
        - "Explain causality in natural language"
      
      context_window: 128000
      system_prompt: |
        You are an expert DevOps engineer performing root cause analysis.
        Analyze logs, metrics, and traces to identify the root cause of incidents.
        Provide confidence scores and supporting evidence.
        Suggest specific remediation actions.
    
    tools:
      - tool: "log_analyzer"
        description: "Parse and analyze log patterns"
        implementation: "custom_python + regex"
      
      - tool: "trace_analyzer"
        description: "Analyze distributed traces for bottlenecks"
        implementation: "jaeger_api"
      
      - tool: "knowledge_base_search"
        description: "Search past incident database"
        implementation: "vector_db_similarity_search"
    
    decision_logic:
      confidence_calculation:
        - "evidence_count * 0.3"
        - "+ log_pattern_match_score * 0.3"
        - "+ similar_past_incident_score * 0.2"
        - "+ llm_confidence * 0.2"
      
      escalation_threshold: 0.7  # If confidence < 0.7, escalate to human
    
    autonomy:
      level: "autonomous_with_confidence_threshold"
      human_in_loop: "if confidence < 0.8"
      escalation: "Escalate low-confidence diagnoses"
  
  # Remediation Agent
  - agent_id: "remediation_agent"
    name: "Healer"
    role: "Automated Remediation & Recovery"
    
    responsibilities:
      - "Receive diagnostic reports from diagnostic agent"
      - "Select appropriate remediation action"
      - "Execute remediation with safety checks"
      - "Verify remediation success"
      - "Rollback if remediation fails"
    
    inputs:
      - source: "message_bus"
        topic: "diagnostics.completed"
        type: "diagnostic_report"
    
    outputs:
      - type: "remediation_result"
        destination: "message_bus"
        topic: "remediation.completed"
        schema:
          incident_id: "string"
          action_taken: "string"
          success: "boolean"
          metrics_after: "object"
          rollback_performed: "boolean"
    
    remediation_actions:
      - action: "restart_service"
        triggers: ["service_crash", "memory_leak", "deadlock"]
        commands:
          - "kubectl rollout restart deployment/{service_name}"
        safety_checks:
          - "Verify service has replicas > 1"
          - "Check no recent restarts in last 10 minutes"
        rollback: "automatic if service doesn't recover in 2 minutes"
      
      - action: "scale_up"
        triggers: ["high_cpu", "high_memory", "high_latency"]
        commands:
          - "kubectl scale deployment/{service_name} --replicas={current + 2}"
        safety_checks:
          - "Max replicas not exceeded"
          - "Check cost budget not exceeded"
        rollback: "Scale down if metrics don't improve in 5 minutes"
      
      - action: "circuit_breaker_reset"
        triggers: ["circuit_breaker_open", "external_service_recovered"]
        commands:
          - "curl -X POST http://{service}/admin/circuit-breaker/reset"
        safety_checks:
          - "Verify upstream service is healthy"
          - "Check error rate is back to normal"
      
      - action: "clear_cache"
        triggers: ["cache_corruption", "stale_data"]
        commands:
          - "redis-cli FLUSHDB"
        safety_checks:
          - "Verify cache is non-critical"
          - "Check cache size is abnormal"
      
      - action: "increase_rate_limit"
        triggers: ["rate_limit_exceeded", "traffic_spike"]
        commands:
          - "Update Kong rate limit via API"
        safety_checks:
          - "Verify not under DDoS attack"
          - "Check upstream can handle increased load"
    
    llm_integration:
      provider: "openai"
      model: "gpt-4"
      use_cases:
        - "Map root cause to remediation action"
        - "Generate custom remediation scripts"
        - "Explain remediation plan before execution"
    
    decision_logic:
      action_selection:
        strategy: "rule_based_with_llm_fallback"
        confidence_threshold: 0.9
        max_attempts: 3
      
      safety_verification:
        - "Check blast radius (affect < 10% of users)"
        - "Verify recent change history (no deploys in last 15 min)"
        - "Confirm remediation has succeeded before (success_rate > 80%)"
    
    autonomy:
      level: "semi_autonomous"
      human_in_loop: true
      approval_required:
        - environment: "production"
          timeout: "5 minutes"
        - environment: "staging"
          timeout: "1 minute"
      auto_approve_conditions:
        - "Non-production environment"
        - "Action has 100% historical success rate"
        - "Blast radius < 1%"
  
  # Learning Agent
  - agent_id: "learning_agent"
    name: "Sage"
    role: "Continuous Learning & Optimization"
    
    responsibilities:
      - "Collect data from all agent interactions"
      - "Analyze incident patterns and outcomes"
      - "Update remediation strategies based on success/failure"
      - "Generate improvement recommendations"
    
    inputs:
      - source: "message_bus"
        topics: ["incidents.*", "diagnostics.*", "remediation.*"]
      
      - source: "incident_database"
        type: "historical_incidents"
        time_range: "last_6_months"
    
    outputs:
      - type: "learning_report"
        destination: "knowledge_base"
        frequency: "weekly"
        schema:
          insights: "array"
          pattern_changes: "array"
          strategy_updates: "array"
      
      - type: "agent_config_update"
        destination: "agent_configs"
        trigger: "significant_pattern_change"
    
    llm_integration:
      provider: "openai"
      model: "gpt-4"
      use_cases:
        - "Identify recurring incident patterns"
        - "Suggest new remediation strategies"
        - "Generate post-mortem insights"
        - "Optimize agent decision thresholds"
    
    learning_mechanisms:
      - mechanism: "Success Rate Tracking"
        description: "Track success rate of each remediation action"
        update_frequency: "daily"
        action: "Demote actions with success_rate < 70%"
      
      - mechanism: "Pattern Recognition"
        description: "Identify new incident patterns using clustering"
        algorithm: "DBSCAN on incident features"
        update_frequency: "weekly"
      
      - mechanism: "Threshold Optimization"
        description: "Optimize anomaly detection thresholds"
        algorithm: "Bayesian optimization"
        objective: "Minimize false positives while maintaining recall > 95%"
    
    autonomy:
      level: "fully_autonomous"
      human_in_loop: false
      reporting: "Weekly summary to team"

# -----------------------------------------------------------------------------
# 5. AGENT ORCHESTRATION
# -----------------------------------------------------------------------------
orchestration:
  pattern: "conductor_based"  # Central coordinator orchestrates agents
  
  message_bus:
    technology: "apache_kafka"
    topics:
      - "incidents.detected"
      - "diagnostics.completed"
      - "remediation.completed"
      - "learning.insights"
    
    message_schema:
      format: "json"
      versioning: true
      validation: "json_schema"
  
  workflow:
    incident_handling:
      trigger: "Anomaly detected by monitoring agent"
      steps:
        - step: 1
          agent: "monitoring_agent"
          action: "Detect anomaly and create incident"
          output: "Incident alert to message bus"
          timeout: "1 minute"
        
        - step: 2
          agent: "diagnostic_agent"
          action: "Analyze incident and determine root cause"
          input: "Incident alert from step 1"
          output: "Diagnostic report"
          timeout: "5 minutes"
        
        - step: 3
          decision: "If confidence >= 0.8, proceed to remediation"
          else: "Escalate to human"
        
        - step: 4
          agent: "remediation_agent"
          action: "Execute remediation"
          input: "Diagnostic report from step 2"
          approval_required: "production only"
          output: "Remediation result"
          timeout: "10 minutes"
        
        - step: 5
          agent: "monitoring_agent"
          action: "Verify incident resolved"
          validation: "Check metrics returned to normal"
        
        - step: 6
          agent: "learning_agent"
          action: "Record incident for learning"
          async: true
  
  coordination:
    coordinator: "workflow_engine"
    technology: "temporal"  # or "airflow", "prefect"
    
    retry_policy:
      max_retries: 3
      backoff: "exponential"
      initial_delay: "30s"
    
    timeout_handling:
      on_timeout: "escalate_to_human"
      notification: "slack + pagerduty"
  
  state_management:
    storage: "postgresql"
    state_tracking:
      - "Current step in workflow"
      - "Agent decisions and outputs"
      - "Human approvals"
      - "Retry attempts"

# -----------------------------------------------------------------------------
# 6. LLM INTEGRATION
# -----------------------------------------------------------------------------
llm_configuration:
  primary_provider: "openai"
  
  models:
    - name: "gpt-4"
      use_case: "Complex reasoning, root cause analysis"
      cost_per_1k_tokens: 0.03
      max_tokens: 8192
    
    - name: "gpt-3.5-turbo"
      use_case: "Simple classifications, summaries"
      cost_per_1k_tokens: 0.002
      max_tokens: 4096
  
  prompt_engineering:
    templates:
      root_cause_analysis: |
        You are an expert DevOps engineer analyzing an incident.
        
        Incident Details:
        - Service: {service_name}
        - Metric: {metric_name}
        - Current Value: {current_value}
        - Normal Range: {normal_range}
        
        Recent Logs:
        {logs}
        
        Recent Metrics:
        {metrics}
        
        Analyze the data and provide:
        1. Most likely root cause
        2. Confidence score (0-1)
        3. Supporting evidence
        4. Recommended remediation action
      
      remediation_plan: |
        Generate a remediation plan for the following incident:
        
        Root Cause: {root_cause}
        Affected Service: {service}
        
        Available Actions: {actions}
        
        Provide:
        1. Recommended action
        2. Step-by-step execution plan
        3. Safety checks to perform
        4. Rollback plan if action fails
    
    few_shot_examples:
      - example: "High memory usage pattern"
        solution: "Restart service to clear memory leak"
      - example: "Database connection pool exhausted"
        solution: "Scale up application replicas"
  
  cost_optimization:
    caching:
      enabled: true
      strategy: "Cache similar queries for 1 hour"
      cache_backend: "redis"
    
    token_limits:
      max_input_tokens: 16000
      max_output_tokens: 2000
    
    fallback:
      on_rate_limit: "Use rule-based system"
      on_timeout: "Use cached response or escalate"

# -----------------------------------------------------------------------------
# 7. KNOWLEDGE BASE & MEMORY
# -----------------------------------------------------------------------------
knowledge_base:
  # Vector database for semantic search
  vector_db:
    technology: "pinecone"  # or "weaviate", "qdrant"
    index: "incident_knowledge"
    embedding_model: "text-embedding-ada-002"
    dimension: 1536
    
    stored_data:
      - type: "past_incidents"
        fields: ["description", "root_cause", "resolution", "outcome"]
        retention: "2 years"
      
      - type: "runbooks"
        fields: ["service", "issue_type", "steps", "success_rate"]
      
      - type: "system_documentation"
        fields: ["service", "architecture", "dependencies"]
  
  # Relational database for structured data
  relational_db:
    technology: "postgresql"
    tables:
      - table: "incidents"
        schema:
          incident_id: "UUID PRIMARY KEY"
          service: "VARCHAR"
          severity: "ENUM"
          root_cause: "TEXT"
          remediation_action: "TEXT"
          resolution_time_minutes: "INTEGER"
          auto_resolved: "BOOLEAN"
          created_at: "TIMESTAMP"
      
      - table: "remediation_actions"
        schema:
          action_id: "UUID PRIMARY KEY"
          action_type: "VARCHAR"
          success_count: "INTEGER"
          failure_count: "INTEGER"
          avg_execution_time_seconds: "FLOAT"
          last_updated: "TIMESTAMP"
  
  # Agent memory
  agent_memory:
    short_term:
      type: "redis"
      ttl: "1 hour"
      stores: "Current workflow state, recent decisions"
    
    long_term:
      type: "postgresql"
      stores: "All incident history, learned patterns"

# -----------------------------------------------------------------------------
# 8. SAFETY & GUARDRAILS
# -----------------------------------------------------------------------------
safety:
  # Circuit breakers for agents
  circuit_breakers:
    - agent: "remediation_agent"
      failure_threshold: 3
      timeout: "30 minutes"
      half_open_after: "15 minutes"
      action_on_open: "Escalate all remediations to human"
  
  # Rate limiting
  rate_limits:
    - agent: "remediation_agent"
      limit: "5 actions per 10 minutes"
      scope: "per_service"
      action_on_exceed: "Queue and require approval"
  
  # Blast radius limits
  blast_radius:
    - action: "restart_service"
      max_affected_users: "10%"
      verification: "Check traffic distribution"
    
    - action: "scale_down"
      min_replicas: 2
      verification: "Ensure high availability maintained"
  
  # Human oversight
  human_oversight:
    always_require_approval:
      - "Production database changes"
      - "Actions affecting > 25% of users"
      - "New remediation actions never seen before"
    
    approval_timeout: "5 minutes"
    on_timeout: "Escalate to on-call engineer"
    
    override_capability: "Human can override agent decision"
  
  # Rollback mechanisms
  rollback:
    automatic_triggers:
      - "Metrics worsen after remediation"
      - "Error rate increases by 50%"
      - "Service becomes unhealthy"
    
    rollback_time: "< 1 minute"
    verification: "Confirm metrics return to baseline"

# -----------------------------------------------------------------------------
# 9. MONITORING & OBSERVABILITY
# -----------------------------------------------------------------------------
monitoring:
  # Agent performance metrics
  agent_metrics:
    - metric: "agent_response_time"
      measurement: "Time from trigger to output"
      threshold:
        monitoring_agent: "< 1 minute"
        diagnostic_agent: "< 5 minutes"
        remediation_agent: "< 10 minutes"
    
    - metric: "agent_accuracy"
      measurement: "Correct decisions / total decisions"
      threshold: ">= 90%"
      evaluation: "Human review of sample decisions"
    
    - metric: "llm_cost"
      measurement: "Total spend on LLM API calls"
      budget: "$500/month"
      alert_threshold: "80% of budget"
  
  # System health metrics
  system_metrics:
    - metric: "auto_resolution_rate"
      target: ">= 60%"
    
    - metric: "false_positive_rate"
      target: "< 5%"
    
    - metric: "mttr"
      target: "< 15 minutes"
  
  # Dashboards
  dashboards:
    - name: "Agent Operations"
      tool: "grafana"
      panels:
        - "Incidents detected per hour"
        - "Auto-resolution rate"
        - "Agent response times"
        - "Remediation success rate"
        - "LLM token usage and cost"
    
    - name: "Learning Progress"
      panels:
        - "New patterns discovered"
        - "Remediation strategy evolution"
        - "Agent confidence over time"

# -----------------------------------------------------------------------------
# 10. TESTING & VALIDATION
# -----------------------------------------------------------------------------
testing:
  # Agent unit tests
  unit_tests:
    framework: "pytest"
    coverage: "85%"
    test_cases:
      - "Anomaly detection logic"
      - "Root cause analysis accuracy"
      - "Remediation action selection"
      - "Safety checks"
  
  # Simulation testing
  simulation:
    tool: "chaos_engineering"
    scenarios:
      - scenario: "High CPU spike"
        expected_agent_response: "Scale up deployment"
        validation: "Verify scaling occurs within 5 minutes"
      
      - scenario: "Service crash"
        expected_agent_response: "Restart service"
        validation: "Service recovers within 2 minutes"
      
      - scenario: "Database connection pool exhausted"
        expected_agent_response: "Increase pool size or scale app"
        validation: "Connection errors stop"
  
  # Human evaluation
  human_eval:
    frequency: "weekly"
    sample_size: "10% of agent decisions"
    metrics:
      - "Decision correctness"
      - "Root cause accuracy"
      - "Remediation appropriateness"

# -----------------------------------------------------------------------------
# 11. SUCCESS CRITERIA
# -----------------------------------------------------------------------------
success_criteria:
  technical:
    - "MTTR reduced from 45 min to < 15 min"
    - "Auto-resolution rate >= 60%"
    - "False positive rate < 5%"
    - "Agent response times within SLA"
  
  business:
    - "System uptime improved to 99.95%"
    - "On-call engineer workload reduced by 50%"
    - "Incident cost reduced by 60%"
  
  operational:
    - "All agents deployed and monitored"
    - "Knowledge base populated with 100+ incidents"
    - "Human approval workflow functional"
    - "Safety guardrails tested and validated"

# -----------------------------------------------------------------------------
# NOTES & IMPLEMENTATION GUIDANCE
# -----------------------------------------------------------------------------
notes: |
  This template defines a production-ready autonomous agentic operations system
  with comprehensive multi-agent communication protocols.
  
  COMMUNICATION ARCHITECTURE:
  1. Hybrid approach combining A2A (gRPC) and Message Bus (Kafka)
  2. Use A2A for latency-sensitive operations (< 100ms requirement)
  3. Use Message Bus for event broadcasting and async workflows
  4. Service mesh (Istio) provides traffic management and observability
  
  PROTOCOL SELECTION GUIDE:
  - A2A gRPC when:
    * Real-time streaming (monitoring → diagnostics)
    * Synchronous request-response needed
    * Low latency critical (< 100ms)
    * Bidirectional communication
  
  - Message Bus (Kafka) when:
    * Multiple subscribers (pub/sub pattern)
    * Event persistence needed for audit
    * Guaranteed delivery required
    * Temporal decoupling of agents
    * Large message payloads
  
  INFRASTRUCTURE REQUIREMENTS:
  - Kubernetes cluster with Istio service mesh
  - Kafka cluster (3 brokers, replication factor 3)
  - etcd cluster for distributed coordination
  - Redis for distributed locking and caching
  - Schema Registry for Avro/Protobuf schemas
  - Jaeger for distributed tracing
  - Prometheus for metrics collection
  
  SECURITY HIGHLIGHTS:
  - Mutual TLS (mTLS) for all agent-to-agent communication
  - Message signing for critical operations (remediation, escalation)
  - RBAC for authorization
  - TLS 1.3 encryption in transit
  - AES-256-GCM encryption at rest
  
  OBSERVABILITY:
  - Distributed tracing with W3C Trace Context propagation
  - Protocol-specific metrics (gRPC latency, Kafka lag)
  - Structured JSON logging with correlation IDs
  - 10% trace sampling for performance
  
  IMPLEMENTATION TIMELINE:
  - Week 1-2: Set up communication infrastructure (Kafka, etcd, Redis)
  - Week 3-4: Implement agent base classes with protocol support
  - Week 5-6: Deploy individual agents with communication wiring
  - Week 7-8: Implement workflows and test end-to-end
  - Week 9-10: Performance tuning and production hardening
  
  COST ESTIMATE:
  - Kafka cluster (3 nodes): ~$500/month
  - etcd cluster (3 nodes): ~$300/month
  - Redis cluster: ~$200/month
  - Service mesh overhead: ~20% additional compute
  - Total infrastructure: ~$2-3K/month (excluding agent compute)

# -----------------------------------------------------------------------------
# TEMPLATE VERSION
# -----------------------------------------------------------------------------
template_version: "2.0.0"
template_last_updated: "2026-02-16"

# Changelog:
# v2.0.0 (2026-02-16):
#   - Added comprehensive multi-agent communication protocols section
#   - Included A2A (gRPC) and Message Bus (Kafka) configurations
#   - Added protocol selection rules and workflow examples
#   - Included security, observability, and performance specifications
#   - Added coordination mechanisms (consensus, leader election, distributed locks)
#   - Defined serialization formats and schema management
# v1.0.0 (2026-01-15):
#   - Initial agentic operations pipeline template
