# =============================================================================
# Data Pipeline Requirements Template
# =============================================================================
# Specialized template for ETL/ELT data pipelines, batch/stream processing,
# data warehousing, and data lake architectures.
# =============================================================================

# -----------------------------------------------------------------------------
# 1. PROJECT METADATA
# -----------------------------------------------------------------------------
project:
  name: "customer-360-data-pipeline"
  version: "1.0.0"
  description: |
    End-to-end data pipeline to build unified customer 360 view by
    ingesting data from CRM, product analytics, support systems, and
    payment platforms. Aggregates to data warehouse for BI and ML.
  
  pipeline_type: "batch_and_streaming"  # batch | streaming | batch_and_streaming | real_time
  architecture: "lambda"                 # lambda | kappa | delta | medallion
  
  domain: "customer_analytics"
  priority: "high"
  deadline: "2026-06-30"
  
  stakeholders:
    product_owner: "data-platform@company.com"
    data_engineer: "data-eng@company.com"
    analytics_lead: "analytics@company.com"
    team: "data-engineering"

# -----------------------------------------------------------------------------
# 2. BUSINESS REQUIREMENTS
# -----------------------------------------------------------------------------
business_requirements:
  objective: |
    Enable 360-degree customer view for:
    - Real-time personalization (marketing)
    - Customer health scoring (success team)
    - Churn prediction (ML models)
    - Executive dashboards (BI reporting)
  
  success_metrics:
    - metric: "data_freshness"
      target: "Batch data < 1 hour latency, Streaming < 5 minutes"
      measurement: "Pipeline execution time + data availability"
    
    - metric: "data_quality"
      target: ">= 99% accuracy on validation rules"
      measurement: "Great Expectations test pass rate"
    
    - metric: "cost_efficiency"
      target: "Process 10TB/month under $5K infrastructure cost"
      measurement: "Monthly cloud billing"
    
    - metric: "uptime"
      target: ">= 99.9% pipeline availability"
      measurement: "Successful runs / total scheduled runs"
  
  use_cases:
    - use_case: "Marketing Personalization"
      users: "Marketing team"
      requirement: "Real-time customer attributes within 5 minutes"
      data_needed: ["demographics", "behavior", "preferences"]
    
    - use_case: "Customer Health Scoring"
      users: "Customer success team"
      requirement: "Daily customer health scores by 8 AM"
      data_needed: ["usage_metrics", "support_tickets", "payment_history"]
    
    - use_case: "Executive Dashboards"
      users: "Executive team"
      requirement: "Daily KPIs refreshed by 7 AM"
      data_needed: ["revenue", "active_users", "churn_rate"]

# -----------------------------------------------------------------------------
# 3. DATA SOURCES
# -----------------------------------------------------------------------------
data_sources:
  # Source systems
  sources:
    - name: "salesforce_crm"
      type: "api"
      technology: "REST API"
      authentication: "oauth2"
      
      entities:
        - entity: "accounts"
          schema: "salesforce.accounts"
          primary_key: "account_id"
          update_frequency: "15 minutes"
          ingestion_mode: "incremental"
          incremental_column: "last_modified_date"
          estimated_volume: "500K records"
        
        - entity: "contacts"
          schema: "salesforce.contacts"
          primary_key: "contact_id"
          update_frequency: "15 minutes"
          ingestion_mode: "incremental"
          estimated_volume: "2M records"
      
      rate_limits:
        requests_per_hour: 100000
        concurrent_connections: 25
      
      availability: "99.9% SLA from vendor"
    
    - name: "product_analytics_db"
      type: "database"
      technology: "PostgreSQL 15"
      authentication: "ssl_certificate"
      connection_string: "postgresql://analytics-db.company.com:5432/prod"
      
      entities:
        - entity: "events"
          schema: "public.user_events"
          primary_key: "event_id"
          update_frequency: "real-time"
          ingestion_mode: "streaming"
          estimated_volume: "10M events/day"
          partitioning: "date"
        
        - entity: "sessions"
          schema: "public.user_sessions"
          primary_key: "session_id"
          update_frequency: "hourly"
          ingestion_mode: "incremental"
          estimated_volume: "500K sessions/day"
    
    - name: "stripe_payments"
      type: "webhook"
      technology: "HTTP webhooks"
      authentication: "signature_validation"
      
      entities:
        - entity: "payments"
          event_type: "payment.succeeded"
          update_frequency: "real-time"
          ingestion_mode: "streaming"
          estimated_volume: "50K payments/day"
    
    - name: "support_tickets_csv"
      type: "file"
      technology: "CSV files"
      location: "s3://company-data/support-exports/"
      
      entities:
        - entity: "tickets"
          file_pattern: "tickets_YYYYMMDD.csv"
          update_frequency: "daily"
          ingestion_mode: "full_refresh"
          estimated_volume: "10K tickets/day"
          delimiter: ","
          encoding: "utf-8"
  
  # Data contracts (schema specifications)
  data_contracts:
    - source: "salesforce_crm"
      entity: "accounts"
      schema_version: "v1.0"
      fields:
        - name: "account_id"
          type: "string"
          required: true
          pii: false
        - name: "account_name"
          type: "string"
          required: true
          max_length: 255
        - name: "industry"
          type: "string"
          required: false
          allowed_values: ["Technology", "Finance", "Healthcare", "Retail", "Other"]
        - name: "annual_revenue"
          type: "decimal"
          required: false
          min_value: 0
        - name: "employee_count"
          type: "integer"
          required: false
          min_value: 1
        - name: "last_modified_date"
          type: "timestamp"
          required: true
          format: "ISO 8601"
      
      sla:
        freshness: "15 minutes"
        completeness: "95%"

# -----------------------------------------------------------------------------
# 4. DATA TRANSFORMATION REQUIREMENTS
# -----------------------------------------------------------------------------
transformations:
  # Transformation layers (Medallion architecture)
  layers:
    bronze:
      description: "Raw data ingestion (exact copy from source)"
      storage: "s3://data-lake/bronze/"
      format: "parquet"
      partitioning: "date"
      retention: "90 days"
      operations:
        - "Schema validation"
        - "Deduplication by primary key"
        - "Add ingestion metadata (timestamp, source, version)"
    
    silver:
      description: "Cleaned and conformed data"
      storage: "s3://data-lake/silver/"
      format: "parquet"
      partitioning: "date"
      retention: "2 years"
      operations:
        - "Data type casting"
        - "Null handling (fill, drop, or flag)"
        - "Standardize naming conventions (snake_case)"
        - "Remove duplicates"
        - "Apply business rules"
        - "Add surrogate keys"
    
    gold:
      description: "Business-level aggregates and entities"
      storage: "s3://data-lake/gold/"
      format: "parquet"
      partitioning: "date"
      retention: "5 years"
      operations:
        - "Join across sources to create customer_360 table"
        - "Calculate KPIs and metrics"
        - "Apply dimension modeling (facts, dimensions)"
        - "Create pre-aggregated tables for BI"
  
  # Specific transformation logic
  transformation_rules:
    - name: "customer_360_creation"
      description: "Join customer data across all sources"
      inputs:
        - "silver.crm_accounts"
        - "silver.product_events_aggregated"
        - "silver.payment_history"
        - "silver.support_tickets_aggregated"
      output: "gold.customer_360"
      join_key: "customer_id"
      logic: |
        SELECT 
          a.customer_id,
          a.account_name,
          a.industry,
          a.annual_revenue,
          e.total_events_last_30d,
          e.last_active_date,
          p.lifetime_value,
          p.last_payment_date,
          s.open_tickets_count,
          s.avg_resolution_time_hours
        FROM silver.crm_accounts a
        LEFT JOIN silver.product_events_aggregated e ON a.customer_id = e.customer_id
        LEFT JOIN silver.payment_history p ON a.customer_id = p.customer_id
        LEFT JOIN silver.support_tickets_aggregated s ON a.customer_id = s.customer_id
        WHERE a.is_active = true
      
      schedule: "hourly"
      sla: "Complete within 30 minutes"
    
    - name: "daily_revenue_aggregation"
      description: "Daily revenue rollup for executive dashboard"
      inputs:
        - "silver.payments"
      output: "gold.daily_revenue"
      logic: |
        SELECT 
          DATE(payment_date) as date,
          COUNT(DISTINCT customer_id) as paying_customers,
          SUM(amount) as total_revenue,
          AVG(amount) as avg_transaction_value,
          COUNT(*) as transaction_count
        FROM silver.payments
        WHERE status = 'succeeded'
        GROUP BY DATE(payment_date)
      
      schedule: "daily at 6 AM"
      sla: "Complete by 7 AM"
  
  # Data quality rules
  data_quality:
    validation_tool: "great_expectations"
    
    validation_rules:
      - table: "silver.crm_accounts"
        rules:
          - expectation: "expect_column_values_to_not_be_null"
            column: "customer_id"
          - expectation: "expect_column_values_to_be_unique"
            column: "customer_id"
          - expectation: "expect_column_values_to_be_in_set"
            column: "industry"
            value_set: ["Technology", "Finance", "Healthcare", "Retail", "Other"]
      
      - table: "gold.customer_360"
        rules:
          - expectation: "expect_table_row_count_to_be_between"
            min_value: 400000
            max_value: 600000
          - expectation: "expect_column_values_to_not_be_null"
            column: "customer_id"
          - expectation: "expect_column_mean_to_be_between"
            column: "lifetime_value"
            min_value: 500
            max_value: 50000
    
    failure_handling:
      critical_failure: "halt_pipeline_and_alert"
      warning: "log_and_continue"
      notification_channel: "slack://data-quality-alerts"

# -----------------------------------------------------------------------------
# 5. PIPELINE ORCHESTRATION
# -----------------------------------------------------------------------------
orchestration:
  tool: "apache_airflow"  # or "dagster", "prefect", "aws_step_functions"
  version: "2.8.0"
  deployment: "kubernetes"
  
  # DAG specifications
  dags:
    - name: "customer_360_batch_pipeline"
      description: "Hourly batch processing for customer 360"
      schedule: "0 * * * *"  # Every hour
      start_date: "2026-03-01"
      catchup: false
      max_active_runs: 1
      concurrency: 10
      
      tasks:
        - task_id: "extract_salesforce"
          type: "python_operator"
          retries: 3
          retry_delay: "5 minutes"
          timeout: "30 minutes"
          dependencies: []
        
        - task_id: "extract_product_db"
          type: "python_operator"
          retries: 3
          retry_delay: "5 minutes"
          timeout: "30 minutes"
          dependencies: []
        
        - task_id: "validate_bronze_data"
          type: "great_expectations_operator"
          dependencies: ["extract_salesforce", "extract_product_db"]
        
        - task_id: "transform_to_silver"
          type: "spark_submit_operator"
          executor_memory: "8g"
          num_executors: 4
          dependencies: ["validate_bronze_data"]
        
        - task_id: "create_customer_360"
          type: "spark_submit_operator"
          executor_memory: "16g"
          num_executors: 8
          dependencies: ["transform_to_silver"]
        
        - task_id: "validate_gold_data"
          type: "great_expectations_operator"
          dependencies: ["create_customer_360"]
        
        - task_id: "load_to_warehouse"
          type: "python_operator"
          dependencies: ["validate_gold_data"]
        
        - task_id: "update_bi_cache"
          type: "http_operator"
          endpoint: "https://bi-tool.company.com/api/refresh"
          dependencies: ["load_to_warehouse"]
      
      sla: "50 minutes"
      sla_notification: "pagerduty"
    
    - name: "daily_aggregations"
      description: "Daily rollups for executive dashboards"
      schedule: "0 6 * * *"  # 6 AM daily
      tasks:
        - task_id: "daily_revenue_agg"
          type: "sql_operator"
        - task_id: "daily_user_metrics"
          type: "sql_operator"
        - task_id: "export_to_bi"
          type: "python_operator"
  
  # Error handling
  error_handling:
    on_failure:
      - "Send alert to Slack #data-eng-alerts"
      - "Create PagerDuty incident if critical"
      - "Log error details to monitoring system"
    
    retry_policy:
      max_retries: 3
      retry_delay: "5 minutes"
      exponential_backoff: true
    
    circuit_breaker:
      failure_threshold: 5
      timeout: "1 hour"
      half_open_retries: 2

# -----------------------------------------------------------------------------
# 6. STORAGE & INFRASTRUCTURE
# -----------------------------------------------------------------------------
storage:
  # Data lake
  data_lake:
    provider: "aws_s3"
    buckets:
      - name: "data-lake-bronze"
        purpose: "Raw ingested data"
        storage_class: "STANDARD"
        lifecycle_rules:
          - transition_to: "STANDARD_IA"
            after_days: 30
          - transition_to: "GLACIER"
            after_days: 90
        versioning: true
        encryption: "AES-256"
      
      - name: "data-lake-silver"
        purpose: "Cleaned and conformed data"
        storage_class: "STANDARD"
        lifecycle_rules:
          - transition_to: "GLACIER"
            after_days: 730  # 2 years
      
      - name: "data-lake-gold"
        purpose: "Business-level aggregates"
        storage_class: "STANDARD"
        lifecycle_rules:
          - archive_to: "GLACIER_DEEP_ARCHIVE"
            after_days: 1825  # 5 years
  
  # Data warehouse
  data_warehouse:
    technology: "snowflake"
    warehouse_size: "LARGE"
    auto_suspend: "10 minutes"
    auto_resume: true
    
    databases:
      - name: "ANALYTICS_PROD"
        schemas:
          - name: "CUSTOMER_360"
            tables:
              - "DIM_CUSTOMERS"
              - "FACT_EVENTS"
              - "FACT_PAYMENTS"
          - name: "REPORTING"
            tables:
              - "DAILY_REVENUE"
              - "CUSTOMER_HEALTH_SCORES"
    
    clustering_keys:
      - table: "FACT_EVENTS"
        keys: ["event_date", "customer_id"]
      - table: "FACT_PAYMENTS"
        keys: ["payment_date", "customer_id"]
  
  # Compute resources
  compute:
    spark_cluster:
      provider: "aws_emr"
      master_instance: "m5.xlarge"
      core_instances:
        instance_type: "r5.2xlarge"
        min_instances: 2
        max_instances: 20
      spot_instances: true
      spot_percentage: 70
      
      configuration:
        spark_version: "3.5.0"
        executor_memory: "16g"
        executor_cores: 4
        driver_memory: "8g"

# -----------------------------------------------------------------------------
# 7. STREAMING PIPELINE (if applicable)
# -----------------------------------------------------------------------------
streaming:
  enabled: true
  
  # Streaming platform
  platform: "apache_kafka"
  version: "3.6"
  brokers: 3
  replication_factor: 3
  
  # Topics
  topics:
    - name: "user-events"
      partitions: 12
      retention: "7 days"
      cleanup_policy: "delete"
      producers: ["product-analytics-app"]
      consumers: ["stream-processor", "analytics-service"]
    
    - name: "payment-events"
      partitions: 6
      retention: "30 days"
      cleanup_policy: "compact"
      producers: ["stripe-webhook-handler"]
      consumers: ["stream-processor", "fraud-detection"]
  
  # Stream processing
  stream_processing:
    framework: "apache_flink"  # or "spark_structured_streaming", "ksqldb"
    version: "1.18"
    
    jobs:
      - name: "real_time_customer_attributes"
        description: "Update customer attributes in real-time"
        parallelism: 12
        checkpoint_interval: "1 minute"
        
        source:
          type: "kafka"
          topic: "user-events"
          starting_offset: "latest"
        
        transformations:
          - "Parse JSON events"
          - "Session windowingg (5 minutes)"
          - "Aggregate events per customer"
          - "Join with dimension tables from Redis"
        
        sink:
          type: "redis"
          key_pattern: "customer:{customer_id}:attributes"
          ttl: "30 days"
        
        sla:
          end_to_end_latency: "< 5 seconds (p95)"
          throughput: "10K events/second"

# -----------------------------------------------------------------------------
# 8. DATA GOVERNANCE & SECURITY
# -----------------------------------------------------------------------------
governance:
  # Data catalog
  data_catalog:
    tool: "aws_glue_catalog"  # or "datahub", "amundsen", "alation"
    auto_discovery: true
    metadata_sync_frequency: "hourly"
    
    documentation_requirements:
      - "Table descriptions"
      - "Column descriptions and business definitions"
      - "Data owners and stewards"
      - "Upstream/downstream lineage"
      - "Data quality SLAs"
  
  # Data lineage
  lineage_tracking:
    tool: "openlineage"  # or "marquez", "datahub"
    track:
      - "Source to bronze"
      - "Bronze to silver transformations"
      - "Silver to gold aggregations"
      - "Gold to BI tools"
    
    visualization: "integrated_in_airflow_ui"
  
  # Access control
  access_control:
    authentication: "okta_sso"
    authorization: "role_based_access_control"
    
    roles:
      - role: "data_engineer"
        permissions:
          - "read_write_all_layers"
          - "create_drop_tables"
          - "execute_pipelines"
      
      - role: "data_analyst"
        permissions:
          - "read_gold_layer"
          - "read_silver_layer"
          - "execute_queries"
      
      - role: "data_scientist"
        permissions:
          - "read_all_layers"
          - "create_temp_tables"
          - "export_data"
  
  # PII handling
  pii_data:
    classification:
      - column: "email"
        classification: "PII"
        masking: "hash_sha256"
      - column: "phone_number"
        classification: "PII"
        masking: "redact"
      - column: "ip_address"
        classification: "PII"
        masking: "mask_last_octet"
    
    retention_policy:
      - data_type: "PII"
        retention: "3 years"
        deletion_method: "hard_delete"

# -----------------------------------------------------------------------------
# 9. MONITORING & OBSERVABILITY
# -----------------------------------------------------------------------------
monitoring:
  # Pipeline monitoring
  pipeline_metrics:
    - metric: "pipeline_success_rate"
      calculation: "successful_runs / total_runs"
      threshold: ">= 99%"
      alert_if: "< 95% over 24 hours"
    
    - metric: "pipeline_duration"
      measurement: "end_time - start_time"
      threshold: "< 50 minutes for hourly pipeline"
      alert_if: "> 60 minutes"
    
    - metric: "data_freshness"
      calculation: "current_time - max(ingestion_timestamp)"
      threshold: "< 1 hour for batch, < 5 min for streaming"
      alert_if: "Exceeds threshold + 50%"
    
    - metric: "records_processed"
      measurement: "count of records per run"
      threshold: "400K - 600K for customer_360"
      alert_if: "Outside expected range"
  
  # Data quality monitoring
  data_quality_metrics:
    - metric: "validation_pass_rate"
      calculation: "passed_validations / total_validations"
      threshold: ">= 99%"
      alert_if: "< 95%"
    
    - metric: "null_percentage"
      measurement: "null_count / total_count per critical column"
      threshold: "< 5%"
      alert_if: "> 10%"
    
    - metric: "duplicate_records"
      measurement: "count of duplicate primary keys"
      threshold: "0"
      alert_if: "> 0"
  
  # Alerting
  alerts:
    - name: "PipelineFailure"
      condition: "pipeline_run failed"
      severity: "critical"
      notification: "pagerduty + slack"
      oncall_escalation: true
    
    - name: "DataQualityDegraded"
      condition: "validation_pass_rate < 95%"
      severity: "high"
      notification: "slack #data-quality"
    
    - name: "DataFreshnessIssue"
      condition: "data_age > sla_threshold + 50%"
      severity: "medium"
      notification: "slack #data-eng"
  
  # Dashboards
  dashboards:
    - name: "Pipeline Health"
      tool: "grafana"
      panels:
        - "Pipeline success rate (24h)"
        - "Pipeline duration trends"
        - "Records processed per run"
        - "Data freshness by source"
    
    - name: "Data Quality"
      tool: "grafana"
      panels:
        - "Validation pass rate by table"
        - "Null percentage trends"
        - "Schema drift detection"
        - "Data volume anomalies"

# -----------------------------------------------------------------------------
# 10. COST OPTIMIZATION
# -----------------------------------------------------------------------------
cost_optimization:
  strategies:
    - strategy: "Use spot instances for Spark jobs"
      savings: "60-70%"
      risk: "Low (jobs are fault-tolerant)"
    
    - strategy: "Partition pruning in queries"
      savings: "30-40% on compute"
      implementation: "Partition by date, filter on date in queries"
    
    - strategy: "Lifecycle policies for S3"
      savings: "50% on storage"
      implementation: "Move old data to Glacier"
    
    - strategy: "Snowflake auto-suspend"
      savings: "40% on warehouse costs"
      implementation: "Auto-suspend after 10 min inactivity"
  
  budget:
    monthly_budget: "$5000"
    breakdown:
      compute: "$2000"
      storage: "$1500"
      networking: "$500"
      third_party_tools: "$1000"
    
    alerts:
      - threshold: "80% of budget"
        action: "notify_team"
      - threshold: "95% of budget"
        action: "freeze_non_critical_jobs"

# -----------------------------------------------------------------------------
# 11. TESTING STRATEGY
# -----------------------------------------------------------------------------
testing:
  # Unit tests
  unit_tests:
    framework: "pytest"
    coverage: "80%"
    test_cases:
      - "Transformation logic"
      - "Data validation rules"
      - "Helper functions"
  
  # Integration tests
  integration_tests:
    - test: "end_to_end_pipeline"
      description: "Run full pipeline with sample data"
      data_volume: "1000 records per source"
      expected_output: "Verify customer_360 table created with correct joins"
    
    - test: "data_quality_validation"
      description: "Verify all Great Expectations tests pass"
      expected_output: "100% validation pass rate on test data"
  
  # Data tests
  data_tests:
    tool: "dbt_test"  # or custom SQL
    tests:
      - test: "unique"
        model: "customer_360"
        column: "customer_id"
      
      - test: "not_null"
        model: "customer_360"
        columns: ["customer_id", "account_name"]
      
      - test: "relationships"
        model: "fact_events"
        column: "customer_id"
        to: "dim_customers"
        field: "customer_id"

# -----------------------------------------------------------------------------
# 12. SUCCESS CRITERIA
# -----------------------------------------------------------------------------
success_criteria:
  technical:
    - "Pipeline completes within SLA 99% of the time"
    - "Data quality validation pass rate >= 99%"
    - "Data freshness < 1 hour for batch, < 5 min for streaming"
    - "Zero data loss during ingestion"
    - "All tests pass with >= 80% coverage"
  
  business:
    - "Customer 360 view available for 500K+ customers"
    - "BI dashboards refresh daily by 7 AM"
    - "Support real-time personalization with < 5 min latency"
    - "Process 10TB/month under $5K budget"
  
  operational:
    - "Monitoring dashboards deployed and alerting configured"
    - "Data catalog documented with lineage"
    - "Runbooks complete for common failure scenarios"
    - "On-call rotation established with clear escalation"
